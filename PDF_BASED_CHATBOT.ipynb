{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMicPu8jwz7ESZDuOk/Xl8+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Install Packages**"],"metadata":{"id":"fWB0xaoHsB8u"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"nGavcZZ96DFW"},"outputs":[],"source":["!pip install langchain\n","#framework that use LLM and tools to build agent\n","!pip install langchain_openai\n","#bridge between langchain and openai\n","!pip install faiss-cpu\n","#for semantic search in vector database\n","!pip install pypdf\n","#for pdf loading\n","!pip install openai\n","#to access openai model\n","!pip install rank_bm25\n","#to use Bm25 keyword based searching algorithm"]},{"cell_type":"markdown","source":["## **Import Libraries**"],"metadata":{"id":"ytSXDu1UsS1P"}},{"cell_type":"code","source":["from langchain.document_loaders import PyPDFLoader\n","#Loads and extracts text content from PDF files into LangChain-compatible documents.\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","#It tries to split by larger structures first (like paragraphs), and if that fails, it recursively falls back to smaller structures (like sentences, then words, then characters)\n","from langchain.vectorstores import FAISS\n","from langchain.embeddings import OllamaEmbeddings\n","from langchain.chat_models import ChatOpenAI\n","from langchain.chains import ConversationalRetrievalChain\n","#Builds a QA chain that retrieves context from a vector database and maintains conversation history.\n","from langchain.memory import ConversationBufferMemory\n","#Stores and manages dialogue history across multiple turns of conversation.\n"],"metadata":{"id":"1SVSXBnD_LE7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Mounting Google Drive for PDF loading**"],"metadata":{"id":"gwf1vBZLvduD"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"NVQHYDctBgIz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Load Two PDF from Google Drive**"],"metadata":{"id":"SPmJ3JlVwC01"}},{"cell_type":"code","source":["loader1 = PyPDFLoader('/content/drive/MyDrive/Colab Notebooks/Machine_Learning.pdf',)\n","loader2 = PyPDFLoader('/content/drive/MyDrive/Colab Notebooks/Mental_Health.pdf')"],"metadata":{"id":"dlBE8CllAlOe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["docs1 = loader1.load()\n","docs2 = loader2.load()"],"metadata":{"id":"TlrV13oyCHO8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Split the document into chunks**"],"metadata":{"id":"nc7EvaDRwRxk"}},{"cell_type":"code","source":["# Split into chunks\n","splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,\n","    chunk_overlap=100)\n"],"metadata":{"id":"RBeGmqwVCNql"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chunk1 = splitter.split_documents(docs1)\n","chunk2 = splitter.split_documents(docs2)"],"metadata":{"id":"qcSp0-4uwy6m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Analyzing Chunks**"],"metadata":{"id":"hND2WbFZw5Mn"}},{"cell_type":"code","source":["print(\"Length of the first chunk=\",   len(chunk1))\n","print(\"Length of the second chunk=\", len(chunk2))"],"metadata":{"id":"41Lc9BTxw_Mt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(chunk1)"],"metadata":{"id":"ihG8k1S3w_1o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for chunk in chunk1:\n","  print(chunk.page_content)\n"],"metadata":{"id":"m7Pt6rXTxAMl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i , chunk in enumerate(chunk1):\n","  print(f\"Chunk {i+1}:\\n {chunk.page_content} \\n\")\n","  print(\"-\"*100)"],"metadata":{"id":"NBSFPa8AxAXs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(chunk2)"],"metadata":{"id":"W7h9RM1LxAiY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for chunk in chunk2:\n","  print(chunk.page_content)"],"metadata":{"id":"uUMvCi9rxArB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i , chunk in enumerate(chunk2):\n","  print(f\"Chunk {i+1}:\\n {chunk.page_content}\\n\")\n","  print(\"=\"*100)"],"metadata":{"id":"TGjquLwSxA3v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Combining two PDF to create one Chunk**"],"metadata":{"id":"2quw_de_xpQh"}},{"cell_type":"code","source":["chunk_document = chunk1 + chunk2"],"metadata":{"id":"peKkJwNBCSjo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(chunk_document)"],"metadata":{"id":"urVRqvMAxmV_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Length of chunk document\", len(chunk_document))"],"metadata":{"id":"GHlAlaMHx09C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for chunk in chunk_document:\n","  print(chunk.page_content)"],"metadata":{"id":"ny8ANLw_x1W9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i, chunk in enumerate(chunk_document):\n","  print(f\"Chunk {i+1} : \\n {chunk.page_content}\\n\")\n","  print(\"+\"*100)"],"metadata":{"id":"UmSbpOGYyGd1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for chunk in chunk_document:\n","  print(chunk.metadata)"],"metadata":{"id":"svqOzbmqyGtM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Create Ollama Embedding model**"],"metadata":{"id":"BD3Dnuf-yHKK"}},{"cell_type":"code","source":["#ollama installation\n","!sudo apt update\n","!sudo apt install -y pciutils\n","!curl -fsSL https://ollama.com/install.sh | sh"],"metadata":{"collapsed":true,"id":"mCpThz8eySQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import threading\n","import subprocess\n","import time\n","\n","def run_ollama_serve():\n","  subprocess.Popen([\"ollama\",\"serve\"])\n","\n","thread=threading.Thread(target=run_ollama_serve)\n","thread.start()\n","time.sleep(5)"],"metadata":{"id":"SX90I5R-ySbZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install ollama"],"metadata":{"id":"yvE9nvce2AqE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ollama pull all-minilm"],"metadata":{"id":"EBg82gQl2A1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_model=OllamaEmbeddings(model=\"all-minilm\")"],"metadata":{"id":"-e2fSygu2NgV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Create a Semantic Vector Store to convert text into vector**"],"metadata":{"id":"4QrRTCZm2Bhc"}},{"cell_type":"code","source":["#convert text into vectors\n","semantic_vector_store = FAISS.from_documents(chunk_document, embedding_model)"],"metadata":{"id":"bs3RTisGCdmq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#save it to local disk\n","semantic_vector_store.save_local(\"faiss_index_semantic_vectorstore\")"],"metadata":{"id":"Pa9J0CmpCequ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load FAISS and LLM\n","semantic_vectorstore = FAISS.load_local(\n","    \"faiss_index_semantic_vectorstore\",\n","    embedding_model,\n","    allow_dangerous_deserialization=True)"],"metadata":{"id":"rWPv0v2VCq4L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Creating Semantic Retriever**"],"metadata":{"id":"PA0cWBg97xWh"}},{"cell_type":"code","source":["semantic_retriever = semantic_vectorstore.as_retriever(\n","    search_kwargs={\n","        \"k\": 5,\n","        \"filter\": {\n","            \"source\": {\"$in\": [\"machine_learning\", \"mental_health\"]}\n","            #metada filtering\n","        }\n","    }\n",")"],"metadata":{"id":"tpuUJzBi7xse"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Create Memory**"],"metadata":{"id":"To5BdN-o4pax"}},{"cell_type":"markdown","source":["**ConversationBufferMemory** stores the full conversation history in memory and feeds it into the LLM as context to enable multi-turn conversations.<br>\n","<br>\n","**memory_key=\"chat_history\"**<br>\n","‚û§ This is the key used to store and retrieve previous messages in the conversation dictionary passed to the chain.\n","<br>\n","<br>\n","**return_messages=True**<br>\n","‚û§ This returns the conversation as ChatMessage objects ({\"role\": \"user\", \"content\": ...}), not just as one big string ‚Äî better for chat models like GPT.\n","<br>\n","<br>\n","**output_key=\"answer\"**<br>\n","‚û§ This tells memory to look for the LLM‚Äôs answer under the key \"answer\" in the chain‚Äôs output so it can append it to the history.\n","\n","\n"],"metadata":{"id":"X26N7yH_5XXe"}},{"cell_type":"code","source":["memory = ConversationBufferMemory(\n","    memory_key=\"chat_history\",\n","    return_messages=True,\n","    output_key=\"answer\")"],"metadata":{"id":"wtFQrlCbCsVd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Creating LLM**"],"metadata":{"id":"u4FwOBZI6geV"}},{"cell_type":"code","source":["import os\n","\n","token=input(\"Enter your Github Token : \")\n","os.environ[\"GITHUB_TOKEN\"]=token\n","model_name=\"openai/gpt-4.1-nano\"\n","endpoint= \"https://models.github.ai/inference\"\n","\n","llm=ChatOpenAI(\n","    model=model_name,\n","    api_key=token,\n","    base_url=endpoint,\n","    temperature=0.7\n",")"],"metadata":{"id":"FA7xphkSCxel"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Create lexical store for keyword retriever**"],"metadata":{"id":"NPGjyPRJ_gcC"}},{"cell_type":"markdown","source":["**BM25Retriever**:<br>\n","-->smart keyword search engine<br>\n","-->No need for word embeddings or AI model<br>\n","-->find most relevant chunks of text based using BM25 algorithm<br>\n","-->previously we do manual tokenizing,bm-scoring,sorting ,listing top k but this retriever includes all internally\n","<br>\n","<br>\n"],"metadata":{"id":"yQ8DoW-tTs-G"}},{"cell_type":"code","source":["from langchain.retrievers import BM25Retriever\n","\n","#Create keyword based  Retriever\n","keyword_retriever = BM25Retriever.from_documents(chunk_document)\n","keyword_retriever.k = 5"],"metadata":{"id":"5Vf0T4yo_hFK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Create Hybrid Retriever for hybrid search**"],"metadata":{"id":"cRVH5yDHBzBr"}},{"cell_type":"markdown","source":["**EnsembeleRetriever:**<br>\n","----combines two or more retriver<br>\n","----weights control the importance of each retriever<br>\n","----normalize scores and return top matchings"],"metadata":{"id":"Pw_xUaFOVnSY"}},{"cell_type":"code","source":["from langchain.retrievers import EnsembleRetriever\n","\n","hybrid_retriever = EnsembleRetriever(\n","    retrievers=[keyword_retriever, semantic_retriever],\n","    weights=[0.5, 0.5]\n",")"],"metadata":{"id":"TkZlcltIAFqy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Create a Chain that connects all**"],"metadata":{"id":"JQ8Go1aD8JPb"}},{"cell_type":"code","source":["final_answer_chain = ConversationalRetrievalChain.from_llm(\n","    llm=llm,\n","    #generate answers\n","    retriever=hybrid_retriever,\n","    #find relevant document\n","    memory=memory,\n","    #keeps track of previous question\n","    return_source_documents=True,\n","    #return chunks that were used to answer the question\n","    output_key=\"answer\"\n",")\n"],"metadata":{"id":"NnSDZtIADIVM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Testing**"],"metadata":{"id":"ECdzZSi8_Yby"}},{"cell_type":"code","source":["questions_document=[\n","    \"What is Machine Learning?\",\n","    \"What are the types of it?\",\n","    \"Tell me about its algorithms?\",\n","    \"Which algorithm is suitable for supervised Learning?\",\n","    \"Tell me which tools and libraries are used for this?\",\n","    \"What is its real world application\",\n","    \"What is Mental Health?\",\n","    \"Tell me its importance\",\n","    \"What could be the common disorders beacause of this?\",\n","    \"What are the causes of Mental Health\",\n","    \"Does it curable?\",\n","    \"What are the treatments need to cure this?\",\n","    \"Are you free from Mental Disorder?\"\n","\n","]"],"metadata":{"id":"pOXfosRLTnZ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for question in questions_document:\n","    response = final_answer_chain.invoke({\"question\": question})\n","    print(f\"‚ùì: {question}\\nüó®Ô∏è: {response['answer']}\\n\")"],"metadata":{"id":"SORkYqkyTrsr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install gradio --quiet"],"metadata":{"id":"g7uH2-CYJiSW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gradio as gr"],"metadata":{"id":"YNs6Ap-HLEUK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Gradio chatbot UI function\n","chat_history = []\n","\n","def chatbot_response(user_input):\n","    global chat_history\n","    if user_input.strip() == \"\":\n","        return chat_history, \"\"\n","    result = qa_chain.invoke({\"question\": user_input})\n","    answer = result['answer']\n","    chat_history.append((\"You\", user_input))\n","    chat_history.append((\"Bot\", answer))\n","    # Format history for Gradio chatbot [(user_msg, bot_msg), ...]\n","    formatted_history = [(chat_history[i][1], chat_history[i+1][1]) for i in range(0, len(chat_history), 2)]\n","    return formatted_history, \"\""],"metadata":{"id":"UaP7H69DKs9d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Build Gradio interface\n","with gr.Blocks() as demo:\n","    gr.Markdown(\"## PDF-based Context-Aware Chatbot\")\n","    with gr.Row():\n","        with gr.Column(scale=1, min_width=200):\n","            chatbot = gr.Chatbot(elem_id=\"chatbot\", height=250)\n","            user_input = gr.Textbox(placeholder=\"Ask a question about the PDFs...\", show_label=False)\n","            clear = gr.Button(\"Clear Chat\")\n","\n","            user_input.submit(chatbot_response, inputs=user_input, outputs=[chatbot, user_input])\n","            clear.click(lambda: ([], \"\"), None, [chatbot, user_input])\n","\n","\n","demo.launch()"],"metadata":{"id":"GljnV09BK6Ji"},"execution_count":null,"outputs":[]}]}